---
layout: post
title: Blog Post 4
---

# Blog Post 4: Spectral Clustering

In this blog post, we will try to implement the *spectral clustering* algorithm for clustering data points. 

### Notation (according to assignment instruction)

In all the math below: 

- Boldface capital letters like $\mathbf{A}$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $\mathbf{v}$ refer to vectors (1d arrays of numbers). 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 

First, let's import some packages that we will need


```python
import numpy as np
from sklearn import datasets # for generating our datasets
from sklearn import metrics # contains some loss, score, and utility functions to measure classification performance
from matplotlib import pyplot as plt
import scipy.optimize as optimize # for minimization of scalar function
```

## Introduction

In this problem, we'll study *spectral clustering*, which is an important tool for identifying meaningful parts of data sets with complex structure. 
First, let's look at an example where we *don't* need spectral clustering. 


```python
n = 200 # the number of samples we want 
np.random.seed(1111) # set seed so we can reproduce the results 
# Generate isotropic Gaussian blobs for clustering
# the Euclidean coordinates of the data points are contained in the matrix X
# the labels of each point are contained in y
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
# let's take a look at the dataset
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x2345114cee0>




    
![png](/images/output_3_1.png)
    


We would like to separate the data set into two natrual *clusterings*.
K-means is a common way to achieve this task, having good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2) # n_clusters: The number of clusters to form
# Compute k-means clustering
km.fit(X)
# take a look at how KMeans has separated the data set into two clusterings
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x234531514c0>




    
![png](/images/output_5_1.png)
    


We can see that KMeans can perform great for these circular-ish blobs.  
However, what if our data set has a weird shape? Let's try a new example:


```python
np.random.seed(1234)
n = 200
# Make two interleaving half circles
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x234531c4e20>




    
![png](/images/output_7_1.png)
    


We can see that there are still two quite obvious cluster in the data. However, they now have the shapes of crescents instead of blobs. Let's see how KMeans work for this example. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x2345322fe20>




    
![png](/images/output_9_1.png)
    


As we see, KMeans can't predict well for this kind of data set.   
Thus, we will now try to implement the spectral clustering algorithm and see it should perform well to correctly cluster the two crescents. 

## Part A

First, we want to construct the *similarity matrix* $\mathbf{A}$.   
##### According to the assignment: 
$\mathbf{A}$ should be a matrix with shape `(n, n)` where `n` is the number of data points.  
We will have a parameter `epsilon`, and the entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` otherwise. We also want the diagonal entries to be zero.
For this part, use `epsilon = 0.4`. 


```python
epsilon = 0.4
# X.shape[0] will give us the number of data points in X
A = np.zeros((X.shape[0], X.shape[0]))
# metrics.pairwise_distances(X) will compute the distance matrix from vector X, 
# i.e. it will compute all the pairwide distances for data points in X
# after finding the distance matrix, we want to set the value of A[i,j] to 1 if i and j are within distance epislon
# metrics.pairwise_distances(X) < epsilon will create the desired mask and we apply the mask to matrix A
A[metrics.pairwise_distances(X) < epsilon] = 1
# we want to set the diagonal values to 0
np.fill_diagonal(A, 0)
# let's take a look at the matrix A
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



## Part B

The matrix `A` now contains information about which points are within distance `epsilon` which other points. 

##### According to the assignment:
we will let $C_0$ and $C_1$ be two clusters of the data points and assume that every data point is in either $C_0$ or $C_1$. `y[i]` being the label of point `i` contains the membership information. So, if `y[i] = 1`, then point `i` is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

#### B.1 The Cut Term

First, we want to compute the cut term $\mathbf{cut}(C_0, C_1)$, which denotes the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$.     
We want the term be small so meaning points in $C_0$ aren't usually very close to points in $C_1$. 

We will write a function called `cut(A,y)` to compute the cut term.


```python
def cut(A,y):
    # boolean array indicating whether data in cluster 0 or not 
    c0 = (y == 0)
    # reshape 
    c0 = c0[:, np.newaxis]
    # we want a mask with the same shape as A with each A[i,j] indicating whether the data i in cluster 0 or not 
    C0 = np.zeros([c0.shape[0],c0.shape[0]])
    C0[:] = c0 # broadcasting 
    # boolean array indicating whether data in cluster 1 or not 
    c1 = (y == 1)
    # we want a mask with the same shape as A with each A[i,j] indicating whether the data j in cluster 1 or not 
    C1 = np.zeros([c1.shape[0],c1.shape[0]])
    C1[:] = c1 # broadcasting
    # making sure both are now boolean matrix
    C0 = C0.astype(bool)
    C1 = C1.astype(bool)
    # combine the two maskings to find entries A[i,j] for each pair of points (i,j) in different clusters
    C = (C0 & C1)
    cut = np.sum(A[C])
    return cut
```

Now, we first want to find the cut value for the true clusters `y`   
Secondly, we want to compare the cut value for `y` to a cut value of a random vector of of random labels of length `n` while the labels are either 0 or 1


```python
cut_y = cut(A, y)
# generate a random vector to compare 
random_vec = np.random.randint(0, 2, X.shape[0])
cut_random = cut(A, random_vec)
# let's see the values of the two cut vales
cut_y, cut_random
```




    (13.0, 1150.0)



Indeed, we can see that the cut value for the true labels is *much* smaller than that of a vector with random labels. 

#### B.2 The Volume Term 
Now we want to write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning them as a tuple.


```python
def vols(A,y):
    # calculate row sum for each row
    row_sum = A.sum(axis=1)
    # for data points in cluster 0
    c0 = (y == 0)
    C0 = np.sum(row_sum[c0])
    # for data points in cluster 1
    c1 = (y == 1)
    C1 = np.sum(row_sum[c1])
    return (C0, C1)
```

Now we want to write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary norm cut objective of a matrix `A` with clustering vector `y`


```python
def normcut(A,y):
    C0, C1  = vols(A,y)
    normcut = cut(A, y) *((1/C0) + (1/C1))
    return normcut
```

Now, we want to compare the `normcut` values for both vector `y` with true labels and also the random vector generated above


```python
normcut_y = normcut(A,y)
normcut_random  = normcut(A,random_vec)
normcut_y, normcut_random
```




    (0.011518412331615225, 1.0240023597759158)



We can see that the normcut value for vector `y` is indeed much smaller than that of the random vector 

## Part C

##### According to the assignment
define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

We now want to write a function called `transform(A,y)` to compute the $\mathbf{z}$ vector given `A` and `y`, using the formula above. 


```python
def transform(A,y):
    z = np.zeros(X.shape[0])
    C0, C1 = vols(A,y)
    # y==0 will create the desire mask for finding points in cluster 0
    # then apply the mask to achieve the desired result according to the formula above
    z[y==0] = 1/C0
    # y==1 will create the desire mask for finding points in cluster 1
    # then apply the mask to achieve the desired result according to the formula above
    z[y==1] = -1/C1
    return z
```

We then want to check the equation above by computing each side separately and checking that they are equal. 


```python
# the left Hand Side
C0, C1  = vols(A,y)
normcut = cut_y *((1/C0) + (1/C1))
# the right hand side
z = transform(A,y)
row_sum = A.sum(axis=1) # calculate the row sum
# matrix D
D = np.zeros((row_sum.shape[0],row_sum.shape[0]))
np.fill_diagonal(D, row_sum)
# D-A
DmA = D - A
# the Right Hand Side
RHS = (z@DmA@z)/(z@D@z)
np.isclose(normcut,RHS)
```




    True



 Now we need to check the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones. 


```python
ones = np.ones(X.shape[0])
check = z@D@ones
check
```




    0.0



## Part D
In the code below, professor defined an `orth_obj` function 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Then we want to use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\mathbf{z}$. 


```python
# scipy.optimize.minimize is for the minimization of scalar function 
# I randomly chose a method from the documentation of this function
# the return value contains an attribute x which is the solution array
z_ = optimize.minimize(orth_obj, z, method='TNC')["x"]
```

## Part E

Now we will plot the original data using whether `z_min[i] < 0` or `z_min[i] >= 0` to determine the color of each point


```python
plt.scatter(X[:,0], X[:,1], c = (z_<0).astype(int)+1)
```




    <matplotlib.collections.PathCollection at 0x234543ac8b0>




    
![png](/images/output_38_1.png)
    


It seems that we are doing a great job at correctly clustering the points into two clusters!

## Part F

##### According to assignment

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

$\mathbb{1}$ is the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

> So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

We thus want to construct the (normalized) *Laplacian* matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$  
Next we need to find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`.        
Then, we will need to plot the data again, using the sign of `z_eig` as the color. 


```python
# construct the matrix L according to the given formula 
L = np.linalg.inv(D)@DmA
# find its eigenvalues and eigenvectors
Lam, U = np.linalg.eig(L)
# we want to sort the eigenvalues and eigenvectors to access the 2nd smallest eigenvalue
ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]
z_eig = U[:,1]
```


```python
plt.scatter(X[:,0], X[:,1], c = (z_eig<0).astype(int)+1)
```




    <matplotlib.collections.PathCollection at 0x23455685d90>




    
![png](/images/output_43_1.png)
    


We can see from the plot that the classification is also pretty good 

## Part G

Now we are ready to write the function `spectral_clustering(X, epsilon)` with input parameters `X` and the distance threshold `epsilon` to perform spectral clustering.   
The return value should be an array of binary labels indicating whether each data point is in group `0` or group `1`. 


```python
def spectral_clustering(X, epsilon):
    """
    Given input parameters X and epsilon, the function should be able to perform spectral clustering on the data set to correctly cluster the points into two groups 
    
    X: a matrix contains the Euclidean coordinates of data points 
    epsilon: distance threshold 
    """
    # Construct the similarity matrix
    A = np.zeros((X.shape[0], X.shape[0]))
    A[metrics.pairwise_distances(X) < epsilon] = 1
    np.fill_diagonal(A, 0)
    # Construct the Laplacian matrix
    D = np.zeros((X.shape[0], X.shape[0]))
    np.fill_diagonal(D, A.sum(axis=1))
    L = np.linalg.inv(D)@(D-A)
    # Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    Lam, U = np.linalg.eig(L)
    Lam, U = Lam[Lam.argsort()], U[:,Lam.argsort()]
    z_eig = U[:,1]
    # Return labels based on this eigenvector
    return (z_eig<0).astype(int)+1
```


```python
# we now apply the function to the supplied data from the beginning problem
labels = spectral_clustering(X, epsilon)
# plot to see the results
plt.scatter(X[:,0], X[:,1], c = labels)
```




    <matplotlib.collections.PathCollection at 0x234556f2ca0>




    
![png](/images/output_47_1.png)
    


## Part H

Now we will generate different data sets using `make_moons` and run experiments with the function created above. 


```python
# this is the no noise version
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0, random_state=None)
labels = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c = labels)
```




    <matplotlib.collections.PathCollection at 0x23452d18190>




    
![png](/images/output_49_1.png)
    



```python
# Now we want to increase the noise and see how the function performs
fig, axarr = plt.subplots(2, 3, figsize = (15, 7))
epsilon = 0.4
for i in range(6):
    row = i // 3
    col = i % 3
    
    noise = [0.03, 0.05, 0.08, 0.13, 0.15, 0.2]
    X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=noise[i], random_state=None)
    labels = spectral_clustering(X, epsilon)
    axarr[row, col].scatter(X[:,0], X[:,1], c = labels)
```


    
![png](/images/output_50_0.png)
    


We can see that the function still works pretty well when we increase noise to different levels. 

## Part I

Now we will try the spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x23452fc67c0>




    
![png](/images/output_53_1.png)
    


There are two concentric circles. We can see that k-means won't perform well. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x23453021fd0>




    
![png](/images/output_55_1.png)
    


Now we want to apply our function to test whether it can successfully separate the two circles?   
We will try different values of `epsilon` between `0` and `1.0`


```python
n = 1000

fig, axarr = plt.subplots(2, 3, figsize = (15, 7))
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)

for i in range(6):
    row = i // 3
    col = i % 3
    
    epsilon = [0.2, 0.4, 0.5, 0.55, 0.6, 0.7]
    labels = spectral_clustering(X, epsilon[i])
    axarr[row, col].scatter(X[:,0], X[:,1], c = labels)
```


    
![png](/images/output_57_0.png)
    


We can see that the function performs well when epsilon is less than 0.5; when epsilon is greater than 0.5, the function fails to separate the two circles.    
That will be all for today!
