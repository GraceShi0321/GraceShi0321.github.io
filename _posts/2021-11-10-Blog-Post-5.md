---
layout: post
title: Blog Post 5
---

In this blog post, we will learn several new skills and concepts related to image classification in Tensorflow.   
We want to build a machine learning model to distinguish between pictures of dogs and pictures of cats.

<h1>§1. Load Packages and Obtain Data</h1>



```python
import os
from tensorflow.keras import utils 
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
import collections
```

First we will need to access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.   
We will create TensorFlow Datasets for training, validation, and testing.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
# The shuffle argument says that, when retrieving data from this directory, the order should be randomized
# The batch_size determines how many data points are gathered from the directory at once. 
# Here each time we request some data we will get 32 images from each of the data sets
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    

### Working with Datasets

Here we will define a function to create a two-row visualization. In the first row, we will show three random pictures of cats. In the second row, we will show three random pictures of dogs.   
We can get a piece of a data set using the take method; e.g. train_dataset.take(1) will retrieve one batch (32 images with labels) from the training data.


```python
def two_row_visualization(train_dataset):
  """
  In the first row, show three random pictures of cats. 
  In the second row, show three random pictures of dogs. 
  """
  class_names = train_dataset.class_names # get the class_names

  plt.figure(figsize=(10, 10))
  for images, labels in train_dataset.take(1):
    # create two lists holding the index we wanted 
    cat_indx = []
    dog_indx = []

    for i in range(len(labels)):  
      # check if the label is cats, if yes, append cat_indx
      if len(cat_indx) < 3:
        if class_names[labels[i]]=="cats":
          cat_indx.append(i)
      # check if the label is dogs, if yes, append dog_indx
      if len(dog_indx) < 3:
        if class_names[labels[i]]=="dogs":
          dog_indx.append(i)
      if (len(cat_indx) == 3) and (len(dog_indx) == 3):
        break
    # concatenate two lists
    index = cat_indx + dog_indx
    # plot the images
    for i in range(6):
      ax = plt.subplot(2, 3, i + 1)
      plt.imshow(images[index[i]].numpy().astype("uint8"))
      plt.title(class_names[labels[index[i]]])
      plt.axis("off")
```

Now, we will use the function to see some visualizations


```python
two_row_visualization(train_dataset)
```


    
![png](/images/output_9_0.png)
    


The following is technical code related to rapidly reading data


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Check Label Frequencies

The following line of code will create an iterator called labels


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

We now want to compute the number of images in the training data with label 0 (corresponding to "cat") and label 1 (corresponding to "dog")


```python
# A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.
counter=collections.Counter(list(labels_iterator))
counter
```




    Counter({0: 1000, 1: 1000})



Since the baseline machine learning model is the model that always guesses the most frequent label, here the baseline accuracy will be 50%.

<h1>§2. First Model</h1>


We will create a tf.keras.Sequential model using some of the layers we’ve discussed in class. We will include Conv2D layers, MaxPooling2D layers, Flatten layer, Dense layers, and Dropout layer in this model


```python
# Sequential building models in keras interface build model by stacking models on each other 
model1 = models.Sequential([
    # beginning (input) learn the base features
    # first layer layers.Conv2D 32 kernels, shape 2*2 'relu' nonlinear transformation, also specify input shape
    layers.Conv2D(32, (2, 2), activation='relu', input_shape=(160, 160, 3)),
    # hidden layers
    # MaxPooling2D((2, 2)) 2*2 the size of window to find the max
    layers.MaxPooling2D((2, 2)),
    # layers.Dropout(0.2) force the model to not fit too closely, help relieve overfitting
    layers.Dropout(0.2),
    layers.Conv2D(32, (2, 2), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # end (output)
    # layers.Flatten(): still have 2d data, now need to reshape into one row
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2) # 2: number of classes
])
```

Now we will train model1 and plot the history of the accuracy on both the training and validation sets. 


```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 81ms/step - loss: 97.3352 - accuracy: 0.5320 - val_loss: 0.6938 - val_accuracy: 0.5000
    Epoch 2/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6951 - accuracy: 0.5600 - val_loss: 0.6938 - val_accuracy: 0.5124
    Epoch 3/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6734 - accuracy: 0.5745 - val_loss: 0.6927 - val_accuracy: 0.5099
    Epoch 4/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6382 - accuracy: 0.6165 - val_loss: 0.6923 - val_accuracy: 0.5309
    Epoch 5/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6050 - accuracy: 0.6420 - val_loss: 0.6962 - val_accuracy: 0.5297
    Epoch 6/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5345 - accuracy: 0.6995 - val_loss: 0.7135 - val_accuracy: 0.5569
    Epoch 7/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.4886 - accuracy: 0.7325 - val_loss: 0.7721 - val_accuracy: 0.5470
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4258 - accuracy: 0.7830 - val_loss: 0.7828 - val_accuracy: 0.5532
    Epoch 9/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3873 - accuracy: 0.8085 - val_loss: 0.8824 - val_accuracy: 0.5347
    Epoch 10/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.3158 - accuracy: 0.8410 - val_loss: 0.9913 - val_accuracy: 0.5446
    Epoch 11/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2752 - accuracy: 0.8660 - val_loss: 1.0724 - val_accuracy: 0.5619
    Epoch 12/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2513 - accuracy: 0.8795 - val_loss: 1.0783 - val_accuracy: 0.5557
    Epoch 13/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2183 - accuracy: 0.8980 - val_loss: 1.2088 - val_accuracy: 0.5631
    Epoch 14/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2272 - accuracy: 0.9200 - val_loss: 1.4078 - val_accuracy: 0.5619
    Epoch 15/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2118 - accuracy: 0.9105 - val_loss: 1.4874 - val_accuracy: 0.5755
    Epoch 16/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.1441 - accuracy: 0.9400 - val_loss: 1.7903 - val_accuracy: 0.5656
    Epoch 17/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.2166 - accuracy: 0.9360 - val_loss: 1.6407 - val_accuracy: 0.5532
    Epoch 18/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.1673 - accuracy: 0.9405 - val_loss: 1.8203 - val_accuracy: 0.5705
    Epoch 19/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.1222 - accuracy: 0.9540 - val_loss: 1.7325 - val_accuracy: 0.5606
    Epoch 20/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.0887 - accuracy: 0.9595 - val_loss: 2.1318 - val_accuracy: 0.5743
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fce4ab79290>




    
![png](/images/output_24_1.png)
    


In order to consistently achieve at least 52% validation accuracy, I have tried using different numbers of layers of the layers.Conv2D and I have also experimented with different kernel sizes. 

**The accuracy of my model stabilized between 52% and 57% during training.**  
The model performed better than the basline by more than 10%.    
We can indeed observe overfitting for this model as the training accuracy is much higher than the validation accuracy. 

<h1>§3. Model with Data Augmentation</h1>


Now we’re going to add some data augmentation layers to the model

First, we will create a tf.keras.layers.RandomFlip() layer.   
Then we will make a plot of the original image and also when RandomFlip() has been applied.


```python
flip = tf.keras.layers.RandomFlip("horizontal")
```


```python
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  for i in range(6):
    ax = plt.subplot(2, 3, i + 1)
    # we want to show the original images for the first row 
    if i < 3:
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.axis('off')
    # we want to show flipped images for second row 
    else:
      fliped_image = flip(tf.expand_dims(images[i-3], 0))
      plt.imshow(fliped_image[0] / 255)
      plt.axis('off')
```


    
![png](/images/output_31_0.png)
    


Then we will create a tf.keras.layers.RandomRotation() layer and make a plot of the original image and also when RandomRotation() has been applied.


```python
rotate = tf.keras.layers.RandomRotation(0.2)
```


```python
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  for i in range(6):
    ax = plt.subplot(2, 3, i + 1)
    # we want to show the original images for the first row 
    if i < 3:
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.axis('off')
    # we want to show rotated images for second row 
    else:
      rotated_image = rotate(tf.expand_dims(images[i-3], 0))
      plt.imshow(rotated_image[0] / 255)
      plt.axis('off')
```


    
![png](/images/output_34_0.png)
    


Now, we are going to create a new tf.keras.models.Sequential model called model2 in which the first two layers are augmentation layers.   
We will use a RandomFlip() layer and a RandomRotation() layer. 


```python
model2 = tf.keras.Sequential([
  # the RandomFlip() layer
  tf.keras.layers.RandomFlip('horizontal'),
  # the RandomRotation() layer
  tf.keras.layers.RandomRotation(0.2),
  # the following is similar to model1
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (2, 2), activation='relu'),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2) # number of classes
])
```

Now we will train model2


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 81ms/step - loss: 68.1625 - accuracy: 0.5100 - val_loss: 0.6902 - val_accuracy: 0.5074
    Epoch 2/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6873 - accuracy: 0.5300 - val_loss: 0.6978 - val_accuracy: 0.5173
    Epoch 3/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6922 - accuracy: 0.5285 - val_loss: 0.6839 - val_accuracy: 0.5210
    Epoch 4/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6811 - accuracy: 0.5650 - val_loss: 0.6955 - val_accuracy: 0.5507
    Epoch 5/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6818 - accuracy: 0.5710 - val_loss: 0.6806 - val_accuracy: 0.5668
    Epoch 6/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6851 - accuracy: 0.5530 - val_loss: 0.6745 - val_accuracy: 0.5842
    Epoch 7/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6786 - accuracy: 0.5785 - val_loss: 0.6703 - val_accuracy: 0.5804
    Epoch 8/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6764 - accuracy: 0.5880 - val_loss: 0.6830 - val_accuracy: 0.5507
    Epoch 9/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6763 - accuracy: 0.5910 - val_loss: 0.6710 - val_accuracy: 0.5965
    Epoch 10/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6718 - accuracy: 0.5935 - val_loss: 0.6732 - val_accuracy: 0.5767
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6636 - accuracy: 0.6050 - val_loss: 0.6523 - val_accuracy: 0.6139
    Epoch 12/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6607 - accuracy: 0.6155 - val_loss: 0.6691 - val_accuracy: 0.5829
    Epoch 13/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6581 - accuracy: 0.6055 - val_loss: 0.6685 - val_accuracy: 0.5854
    Epoch 14/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6598 - accuracy: 0.6140 - val_loss: 0.6600 - val_accuracy: 0.5842
    Epoch 15/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.6505 - accuracy: 0.6270 - val_loss: 0.6503 - val_accuracy: 0.6324
    Epoch 16/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6526 - accuracy: 0.6295 - val_loss: 0.6482 - val_accuracy: 0.6374
    Epoch 17/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6497 - accuracy: 0.6225 - val_loss: 0.6476 - val_accuracy: 0.6473
    Epoch 18/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6442 - accuracy: 0.6335 - val_loss: 0.6486 - val_accuracy: 0.6436
    Epoch 19/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6427 - accuracy: 0.6425 - val_loss: 0.6440 - val_accuracy: 0.6460
    Epoch 20/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6471 - accuracy: 0.6410 - val_loss: 0.6368 - val_accuracy: 0.6374
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fce4efd57d0>




    
![png](/images/output_40_1.png)
    


**The accuracy of this model stabilized between 58% and 63% during training.**  
This model2's validation accuracy performed better than model1 by more than 5%.    
We do not observe overfitting for this model as the trend for training accuracy is cosistent with the trend for the validation accuracy and the values are quite close. 

<h1>§4. Data Preprocessing</h1>

The following code will create a preprocessing layer called preprocessor to make simple transformations to the input data so that model can be trained faster with RGB values normalized 


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, we are going to create a new tf.keras.models.Sequential model called model3 in which the first layer will be the preprocessor layer


```python
model3 = tf.keras.Sequential([
  # we will incorporate the preprocessor layer as the very first layer
  preprocessor,            
  # the data augmentation layers
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
  # below similar to model2
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (2, 2), activation='relu'),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2) # number of classes
])
```

Now we will train model3


```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 82ms/step - loss: 0.7673 - accuracy: 0.5175 - val_loss: 0.6921 - val_accuracy: 0.4926
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6853 - accuracy: 0.5560 - val_loss: 0.6604 - val_accuracy: 0.6101
    Epoch 3/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6673 - accuracy: 0.5825 - val_loss: 0.6445 - val_accuracy: 0.5891
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6432 - accuracy: 0.6125 - val_loss: 0.6250 - val_accuracy: 0.6374
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6248 - accuracy: 0.6495 - val_loss: 0.6307 - val_accuracy: 0.6002
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6171 - accuracy: 0.6525 - val_loss: 0.6176 - val_accuracy: 0.6535
    Epoch 7/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6059 - accuracy: 0.6655 - val_loss: 0.5979 - val_accuracy: 0.6720
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5869 - accuracy: 0.6690 - val_loss: 0.6025 - val_accuracy: 0.6720
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5693 - accuracy: 0.6965 - val_loss: 0.5675 - val_accuracy: 0.7104
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5542 - accuracy: 0.7130 - val_loss: 0.5477 - val_accuracy: 0.7327
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5551 - accuracy: 0.7090 - val_loss: 0.5432 - val_accuracy: 0.7314
    Epoch 12/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5391 - accuracy: 0.7230 - val_loss: 0.5541 - val_accuracy: 0.7215
    Epoch 13/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5360 - accuracy: 0.7345 - val_loss: 0.5256 - val_accuracy: 0.7314
    Epoch 14/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5183 - accuracy: 0.7410 - val_loss: 0.5086 - val_accuracy: 0.7488
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5079 - accuracy: 0.7505 - val_loss: 0.5248 - val_accuracy: 0.7252
    Epoch 16/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4970 - accuracy: 0.7615 - val_loss: 0.5043 - val_accuracy: 0.7562
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4867 - accuracy: 0.7585 - val_loss: 0.5192 - val_accuracy: 0.7327
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4909 - accuracy: 0.7665 - val_loss: 0.4725 - val_accuracy: 0.7686
    Epoch 19/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4764 - accuracy: 0.7750 - val_loss: 0.4908 - val_accuracy: 0.7463
    Epoch 20/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.4731 - accuracy: 0.7685 - val_loss: 0.4757 - val_accuracy: 0.7760
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fce4e719fd0>




    
![png](/images/output_50_1.png)
    


**The accuracy of this model stabilized between 72% and 77% during training.**  
This model3's validation accuracy performed much better than model1 by about 20%.    
We do not observe overfitting for this model as the training accuracy is being consistenly close to the validation accuracy. 

<h1>§5. Transfer Learning</h1>

Here we would like to use a pre-existing model for our task and then further build our model.  
We need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.  
We will use the following code in order to download MobileNetV2


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step
    

Now, we are going to create a new tf.keras.models.Sequential model called model4 in which we will configure MobileNetV2 as a layer


```python
model4 = tf.keras.Sequential([
  # we will incorporate the preprocessor layer as the very first layer
  preprocessor,      
  # the data augmentation layers
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
  # The base_model_layer constructed above
  base_model_layer,
  # below similar to model2
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  # A Dense(2) layer at the very end to actually perform the classification
  layers.Dense(2) # number of classes
])
```

we want to check the model.summary()


```python
model4.summary()
```

    Model: "sequential_10"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_9 (RandomFlip)  (None, 160, 160, 3)       0         
                                                                     
     random_rotation_9 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     conv2d_24 (Conv2D)          (None, 4, 4, 32)          163872    
                                                                     
     max_pooling2d_19 (MaxPoolin  (None, 2, 2, 32)         0         
     g2D)                                                            
                                                                     
     dropout_10 (Dropout)        (None, 2, 2, 32)          0         
                                                                     
     flatten_10 (Flatten)        (None, 128)               0         
                                                                     
     dense_20 (Dense)            (None, 64)                8256      
                                                                     
     dense_21 (Dense)            (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 2,430,242
    Trainable params: 172,258
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

We can see from the above summary that we need to train 2,430,242 paramters in the model.  
This is due to there is a lot of complexity hidden in the base_model_layer

Now we want to train our model4 for 20 epochs


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 118ms/step - loss: 0.2876 - accuracy: 0.8900 - val_loss: 0.0726 - val_accuracy: 0.9616
    Epoch 2/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1407 - accuracy: 0.9405 - val_loss: 0.0491 - val_accuracy: 0.9827
    Epoch 3/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1379 - accuracy: 0.9485 - val_loss: 0.0459 - val_accuracy: 0.9777
    Epoch 4/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1269 - accuracy: 0.9425 - val_loss: 0.0505 - val_accuracy: 0.9790
    Epoch 5/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0933 - accuracy: 0.9610 - val_loss: 0.0366 - val_accuracy: 0.9901
    Epoch 6/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1078 - accuracy: 0.9580 - val_loss: 0.0591 - val_accuracy: 0.9752
    Epoch 7/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0862 - accuracy: 0.9690 - val_loss: 0.0498 - val_accuracy: 0.9814
    Epoch 8/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1121 - accuracy: 0.9560 - val_loss: 0.0592 - val_accuracy: 0.9777
    Epoch 9/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0928 - accuracy: 0.9620 - val_loss: 0.0686 - val_accuracy: 0.9715
    Epoch 10/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0769 - accuracy: 0.9715 - val_loss: 0.0504 - val_accuracy: 0.9802
    Epoch 11/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0549 - accuracy: 0.9785 - val_loss: 0.0487 - val_accuracy: 0.9814
    Epoch 12/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0698 - accuracy: 0.9725 - val_loss: 0.0399 - val_accuracy: 0.9864
    Epoch 13/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0698 - accuracy: 0.9735 - val_loss: 0.0379 - val_accuracy: 0.9802
    Epoch 14/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0795 - accuracy: 0.9730 - val_loss: 0.0753 - val_accuracy: 0.9715
    Epoch 15/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.0705 - accuracy: 0.9735 - val_loss: 0.0481 - val_accuracy: 0.9814
    Epoch 16/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.0459 - val_accuracy: 0.9814
    Epoch 17/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0588 - accuracy: 0.9745 - val_loss: 0.0428 - val_accuracy: 0.9864
    Epoch 18/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0500 - accuracy: 0.9785 - val_loss: 0.0486 - val_accuracy: 0.9814
    Epoch 19/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.0443 - accuracy: 0.9820 - val_loss: 0.0350 - val_accuracy: 0.9864
    Epoch 20/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0430 - accuracy: 0.9850 - val_loss: 0.0357 - val_accuracy: 0.9839
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fce4ec4c550>




    
![png](/images/output_63_1.png)
    


**The accuracy of this model stabilized between 96% and 98% during training.**  
This model4's validation accuracy performed much better than model1 by more than 40%, and it is better than model3 by more than 20%.     
We do not observe overfitting for this model as the validation accuracy performs better than the training accuracy most of the times. 

<h1>§6. Score on Test Data</h1>

Now we would want to evaluate the accuracy of our most performant model, model4, on the unseen test_dataset


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 63ms/step - loss: 0.0669 - accuracy: 0.9896
    Test accuracy : 0.9895833134651184
    

We see that the test accuracy is over 98% which is quite impressive!
That will be all for today~
