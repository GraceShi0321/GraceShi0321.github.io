---
layout: post
title: Blog Post 5
---

In this blog post, we will learn several new skills and concepts related to image classification in Tensorflow.   
We want to build a machine learning model to distinguish between pictures of dogs and pictures of cats.

<h1>§1. Load Packages and Obtain Data</h1>



```python
import os
from tensorflow.keras import utils 
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
import collections
```

First we will need to access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.   
We will create TensorFlow Datasets for training, validation, and testing.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
# The shuffle argument says that, when retrieving data from this directory, the order should be randomized
# The batch_size determines how many data points are gathered from the directory at once. 
# Here each time we request some data we will get 32 images from each of the data sets
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    

### Working with Datasets

Here we will define a function to create a two-row visualization. In the first row, we will show three random pictures of cats. In the second row, we will show three random pictures of dogs.   
We can get a piece of a data set using the take method; e.g. train_dataset.take(1) will retrieve one batch (32 images with labels) from the training data.


```python
def two_row_visualization(train_dataset):
  """
  In the first row, show three random pictures of cats. 
  In the second row, show three random pictures of dogs. 
  """
  class_names = train_dataset.class_names # get the class_names

  plt.figure(figsize=(10, 10))
  for images, labels in train_dataset.take(1):
    # create two lists holding the index we wanted 
    cat_indx = []
    dog_indx = []

    for i in range(len(labels)):  
      # check if the label is cats, if yes, append cat_indx
      if len(cat_indx) < 3:
        if class_names[labels[i]]=="cats":
          cat_indx.append(i)
      # check if the label is dogs, if yes, append dog_indx
      if len(dog_indx) < 3:
        if class_names[labels[i]]=="dogs":
          dog_indx.append(i)
      if (len(cat_indx) == 3) and (len(dog_indx) == 3):
        break
    # concatenate two lists
    index = cat_indx + dog_indx
    # plot the images
    for i in range(6):
      ax = plt.subplot(2, 3, i + 1)
      plt.imshow(images[index[i]].numpy().astype("uint8"))
      plt.title(class_names[labels[index[i]]])
      plt.axis("off")
```

Now, we will use the function to see some visualizations


```python
two_row_visualization(train_dataset)
```


    
![png](/images/output_9_0.png)
    


The following is technical code related to rapidly reading data


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Check Label Frequencies

The following line of code will create an iterator called labels


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

We now want to compute the number of images in the training data with label 0 (corresponding to "cat") and label 1 (corresponding to "dog")


```python
# A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.
counter=collections.Counter(list(labels_iterator))
counter
```




    Counter({0: 1000, 1: 1000})



Since the baseline machine learning model is the model that always guesses the most frequent label, here the baseline accuracy will be 50%.

<h1>§2. First Model</h1>


We will create a tf.keras.Sequential model using some of the layers we’ve discussed in class. We will include Conv2D layers, MaxPooling2D layers, Flatten layer, Dense layers, and Dropout layer in this model


```python
# Sequential building models in keras interface build model by stacking models on each other 
model1 = models.Sequential([
    # beginning (input) learn the base features
    # first layer layers.Conv2D 32 kernels, shape 2*2 'relu' nonlinear transformation, also specify input shape
    layers.Conv2D(32, (2, 2), activation='relu', input_shape=(160, 160, 3)),
    # hidden layers
    # MaxPooling2D((2, 2)) 2*2 the size of window to find the max
    layers.MaxPooling2D((2, 2)),
    # layers.Dropout(0.2) force the model to not fit too closely, help relieve overfitting
    layers.Dropout(0.2),
    layers.Conv2D(32, (2, 2), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # end (output)
    # layers.Flatten(): still have 2d data, now need to reshape into one row
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(2) # 2: number of classes
])
```

Now we will train model1 and plot the history of the accuracy on both the training and validation sets. 


```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 36s 84ms/step - loss: 59.7299 - accuracy: 0.5290 - val_loss: 1.5811 - val_accuracy: 0.5371
    Epoch 2/20
    63/63 [==============================] - 5s 76ms/step - loss: 2.1825 - accuracy: 0.6020 - val_loss: 0.7144 - val_accuracy: 0.5198
    Epoch 3/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.7894 - accuracy: 0.6295 - val_loss: 0.6952 - val_accuracy: 0.5210
    Epoch 4/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.6209 - accuracy: 0.6720 - val_loss: 0.7031 - val_accuracy: 0.5124
    Epoch 5/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5342 - accuracy: 0.7240 - val_loss: 0.7068 - val_accuracy: 0.5235
    Epoch 6/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.4904 - accuracy: 0.7555 - val_loss: 0.7096 - val_accuracy: 0.5235
    Epoch 7/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.4260 - accuracy: 0.7870 - val_loss: 0.7373 - val_accuracy: 0.5248
    Epoch 8/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.3735 - accuracy: 0.8140 - val_loss: 0.7581 - val_accuracy: 0.5681
    Epoch 9/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.3569 - accuracy: 0.8390 - val_loss: 0.8211 - val_accuracy: 0.5668
    Epoch 10/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.3198 - accuracy: 0.8490 - val_loss: 0.8517 - val_accuracy: 0.5656
    Epoch 11/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.2869 - accuracy: 0.8730 - val_loss: 0.9513 - val_accuracy: 0.5854
    Epoch 12/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.2764 - accuracy: 0.8725 - val_loss: 0.9976 - val_accuracy: 0.5730
    Epoch 13/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.2682 - accuracy: 0.8745 - val_loss: 1.1224 - val_accuracy: 0.5866
    Epoch 14/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.2204 - accuracy: 0.8975 - val_loss: 1.1921 - val_accuracy: 0.5965
    Epoch 15/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.2380 - accuracy: 0.8960 - val_loss: 1.2220 - val_accuracy: 0.5854
    Epoch 16/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.2318 - accuracy: 0.9075 - val_loss: 1.2270 - val_accuracy: 0.5842
    Epoch 17/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.2010 - accuracy: 0.9080 - val_loss: 1.3600 - val_accuracy: 0.5631
    Epoch 18/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.1818 - accuracy: 0.9220 - val_loss: 1.4521 - val_accuracy: 0.6027
    Epoch 19/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.1469 - accuracy: 0.9320 - val_loss: 1.6404 - val_accuracy: 0.5842
    Epoch 20/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.1299 - accuracy: 0.9445 - val_loss: 1.5791 - val_accuracy: 0.5854
    

We want to visualize the training history.


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6d93d13390>




    
![png](/images/output_24_1.png)
    


In order to consistently achieve at least 52% validation accuracy, I have tried using different numbers of layers of the layers.Conv2D and I have also experimented with different kernel sizes. 

**The accuracy of my model stabilized between 56% and 60% during training.**  
The model performed better than the basline by more than 5%.    
We can indeed observe overfitting for this model as the training accuracy is much higher than the validation accuracy. 

<h1>§3. Model with Data Augmentation</h1>


Now we’re going to add some data augmentation layers to the model

First, we will create a tf.keras.layers.RandomFlip() layer.   
Then we will make a plot of the original image and also when RandomFlip() has been applied.


```python
RandomFlip = tf.keras.layers.RandomFlip()
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = RandomFlip(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![png](/images/output_31_0.png)
    


Then we will create a tf.keras.layers.RandomRotation() layer and make a plot of the original image and also when RandomRotation() has been applied.


```python
RandomRotation = tf.keras.layers.RandomRotation(0.5)
```


```python
for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = RandomRotation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


    
![png](/images/output_34_0.png)
    


Now, we are going to create a new tf.keras.models.Sequential model called model2 in which the first two layers are augmentation layers.   
We will use a RandomFlip() layer and a RandomRotation() layer. 


```python
model2 = tf.keras.Sequential([
  # the RandomFlip() layer
  tf.keras.layers.RandomFlip('horizontal'),
  # the RandomRotation() layer
  tf.keras.layers.RandomRotation(0.2),
  # the following is similar to model1
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (2, 2), activation='relu'),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2) # number of classes
])
```

Now we will train model2


```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 83ms/step - loss: 40.0841 - accuracy: 0.5180 - val_loss: 0.6941 - val_accuracy: 0.5074
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6925 - val_accuracy: 0.5149
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6902 - accuracy: 0.5305 - val_loss: 0.6988 - val_accuracy: 0.4963
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6874 - accuracy: 0.5265 - val_loss: 0.6966 - val_accuracy: 0.5421
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6871 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5644
    Epoch 6/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6861 - accuracy: 0.5345 - val_loss: 0.6894 - val_accuracy: 0.5260
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6781 - accuracy: 0.5805 - val_loss: 0.6921 - val_accuracy: 0.5507
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6843 - accuracy: 0.5660 - val_loss: 0.6954 - val_accuracy: 0.5359
    Epoch 9/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6794 - accuracy: 0.5640 - val_loss: 0.6963 - val_accuracy: 0.5408
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6775 - accuracy: 0.5645 - val_loss: 0.7023 - val_accuracy: 0.5705
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6684 - accuracy: 0.5790 - val_loss: 0.7022 - val_accuracy: 0.5705
    Epoch 12/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6685 - accuracy: 0.5805 - val_loss: 0.6835 - val_accuracy: 0.5817
    Epoch 13/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6728 - accuracy: 0.5640 - val_loss: 0.6812 - val_accuracy: 0.5532
    Epoch 14/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6664 - accuracy: 0.5835 - val_loss: 0.6688 - val_accuracy: 0.6002
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6556 - accuracy: 0.5970 - val_loss: 0.6771 - val_accuracy: 0.5780
    Epoch 16/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6514 - accuracy: 0.6085 - val_loss: 0.6735 - val_accuracy: 0.5965
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6590 - accuracy: 0.6025 - val_loss: 0.6867 - val_accuracy: 0.5792
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6445 - accuracy: 0.6075 - val_loss: 0.6542 - val_accuracy: 0.6015
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6516 - accuracy: 0.6045 - val_loss: 0.6790 - val_accuracy: 0.5953
    Epoch 20/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6537 - accuracy: 0.6050 - val_loss: 0.6758 - val_accuracy: 0.5941
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6d1b922750>




    
![png](/images/output_40_1.png)
    


**The accuracy of this model stabilized between 57% and 60% during training.**  
This model2's validation accuracy is miliar to that of model1.   
We do not observe overfitting for this model as the trend for training accuracy is cosistent with the trend for the validation accuracy and the values are quite close. 

<h1>§4. Data Preprocessing</h1>

The following code will create a preprocessing layer called preprocessor to make simple transformations to the input data so that model can be trained faster with RGB values normalized 


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, we are going to create a new tf.keras.models.Sequential model called model3 in which the first layer will be the preprocessor layer


```python
model3 = tf.keras.Sequential([
  # we will incorporate the preprocessor layer as the very first layer
  preprocessor,            
  # the data augmentation layers
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
  # below similar to model2
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (2, 2), activation='relu'),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2) # number of classes
])
```

Now we will train model3


```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 82ms/step - loss: 0.7673 - accuracy: 0.5175 - val_loss: 0.6921 - val_accuracy: 0.4926
    Epoch 2/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6853 - accuracy: 0.5560 - val_loss: 0.6604 - val_accuracy: 0.6101
    Epoch 3/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6673 - accuracy: 0.5825 - val_loss: 0.6445 - val_accuracy: 0.5891
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6432 - accuracy: 0.6125 - val_loss: 0.6250 - val_accuracy: 0.6374
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6248 - accuracy: 0.6495 - val_loss: 0.6307 - val_accuracy: 0.6002
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6171 - accuracy: 0.6525 - val_loss: 0.6176 - val_accuracy: 0.6535
    Epoch 7/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6059 - accuracy: 0.6655 - val_loss: 0.5979 - val_accuracy: 0.6720
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5869 - accuracy: 0.6690 - val_loss: 0.6025 - val_accuracy: 0.6720
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5693 - accuracy: 0.6965 - val_loss: 0.5675 - val_accuracy: 0.7104
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5542 - accuracy: 0.7130 - val_loss: 0.5477 - val_accuracy: 0.7327
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5551 - accuracy: 0.7090 - val_loss: 0.5432 - val_accuracy: 0.7314
    Epoch 12/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5391 - accuracy: 0.7230 - val_loss: 0.5541 - val_accuracy: 0.7215
    Epoch 13/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5360 - accuracy: 0.7345 - val_loss: 0.5256 - val_accuracy: 0.7314
    Epoch 14/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5183 - accuracy: 0.7410 - val_loss: 0.5086 - val_accuracy: 0.7488
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5079 - accuracy: 0.7505 - val_loss: 0.5248 - val_accuracy: 0.7252
    Epoch 16/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4970 - accuracy: 0.7615 - val_loss: 0.5043 - val_accuracy: 0.7562
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4867 - accuracy: 0.7585 - val_loss: 0.5192 - val_accuracy: 0.7327
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.4909 - accuracy: 0.7665 - val_loss: 0.4725 - val_accuracy: 0.7686
    Epoch 19/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4764 - accuracy: 0.7750 - val_loss: 0.4908 - val_accuracy: 0.7463
    Epoch 20/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.4731 - accuracy: 0.7685 - val_loss: 0.4757 - val_accuracy: 0.7760
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fce4e719fd0>




    
![png](/images/output_50_1.png)
    


**The accuracy of this model stabilized between 72% and 77% during training.**  
This model3's validation accuracy performed much better than model1 by about 20%.    
We do not observe overfitting for this model as the training accuracy is being consistenly close to the validation accuracy. 

<h1>§5. Transfer Learning</h1>

Here we would like to use a pre-existing model for our task and then further build our model.  
We need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.  
We will use the following code in order to download MobileNetV2


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step
    

Now, we are going to create a new tf.keras.models.Sequential model called model4 in which we will configure MobileNetV2 as a layer


```python
model4 = tf.keras.Sequential([
  # we will incorporate the preprocessor layer as the very first layer
  preprocessor,      
  # the data augmentation layers
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
  # The base_model_layer constructed above
  base_model_layer,
  # below similar to model2
  layers.Conv2D(32, (2, 2), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  # end (output)
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  # A Dense(2) layer at the very end to actually perform the classification
  layers.Dense(2) # number of classes
])
```

we want to check the model.summary()


```python
model4.summary()
```

    Model: "sequential_8"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_10 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_11 (RandomR  (None, 160, 160, 3)      0         
     otation)                                                        
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     conv2d_23 (Conv2D)          (None, 4, 4, 32)          163872    
                                                                     
     max_pooling2d_16 (MaxPoolin  (None, 2, 2, 32)         0         
     g2D)                                                            
                                                                     
     dropout_8 (Dropout)         (None, 2, 2, 32)          0         
                                                                     
     flatten_8 (Flatten)         (None, 128)               0         
                                                                     
     dense_16 (Dense)            (None, 64)                8256      
                                                                     
     dense_17 (Dense)            (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 2,430,242
    Trainable params: 172,258
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

We can see from the above summary that we need to train 2,430,242 paramters in the model.  
This is due to there is a lot of complexity hidden in the base_model_layer

Now we want to train our model4 for 20 epochs


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
# from_logits=True compute softmax when evaluting loss function
# metrics=['accuracy'] want to see how accurate on the data
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 12s 119ms/step - loss: 0.2918 - accuracy: 0.8975 - val_loss: 0.0597 - val_accuracy: 0.9752
    Epoch 2/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1566 - accuracy: 0.9315 - val_loss: 0.0480 - val_accuracy: 0.9839
    Epoch 3/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1250 - accuracy: 0.9545 - val_loss: 0.0494 - val_accuracy: 0.9827
    Epoch 4/20
    63/63 [==============================] - 7s 101ms/step - loss: 0.1109 - accuracy: 0.9555 - val_loss: 0.0435 - val_accuracy: 0.9827
    Epoch 5/20
    63/63 [==============================] - 7s 100ms/step - loss: 0.0975 - accuracy: 0.9665 - val_loss: 0.0577 - val_accuracy: 0.9765
    Epoch 6/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.1090 - accuracy: 0.9540 - val_loss: 0.0484 - val_accuracy: 0.9814
    Epoch 7/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.0988 - accuracy: 0.9575 - val_loss: 0.0476 - val_accuracy: 0.9790
    Epoch 8/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1035 - accuracy: 0.9575 - val_loss: 0.0366 - val_accuracy: 0.9876
    Epoch 9/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1066 - accuracy: 0.9595 - val_loss: 0.0470 - val_accuracy: 0.9814
    Epoch 10/20
    63/63 [==============================] - 7s 99ms/step - loss: 0.0952 - accuracy: 0.9640 - val_loss: 0.0360 - val_accuracy: 0.9827
    Epoch 11/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.0692 - accuracy: 0.9720 - val_loss: 0.0394 - val_accuracy: 0.9864
    Epoch 12/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0695 - accuracy: 0.9745 - val_loss: 0.0425 - val_accuracy: 0.9839
    Epoch 13/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0662 - accuracy: 0.9740 - val_loss: 0.0524 - val_accuracy: 0.9827
    Epoch 14/20
    63/63 [==============================] - 6s 97ms/step - loss: 0.0670 - accuracy: 0.9750 - val_loss: 0.0549 - val_accuracy: 0.9802
    Epoch 15/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.0447 - val_accuracy: 0.9790
    Epoch 16/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.0573 - accuracy: 0.9790 - val_loss: 0.0461 - val_accuracy: 0.9790
    Epoch 17/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.0617 - accuracy: 0.9765 - val_loss: 0.0550 - val_accuracy: 0.9790
    Epoch 18/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.0641 - accuracy: 0.9710 - val_loss: 0.0468 - val_accuracy: 0.9839
    Epoch 19/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.0546 - accuracy: 0.9765 - val_loss: 0.0400 - val_accuracy: 0.9851
    Epoch 20/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.0460 - accuracy: 0.9815 - val_loss: 0.0577 - val_accuracy: 0.9864
    

We want to visualize the training history.


```python
# history object holds a record of the loss values and metric values during training
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy") # gca: get current axis 
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6d939f9210>




    
![png](/images/output_63_1.png)
    


**The accuracy of this model stabilized between 97% and 98% during training.**  
This model4's validation accuracy performed much better than model1 by about 40%, and it is better than model3 by more than 20%.     
We do not observe overfitting for this model as the validation accuracy performs better than the training accuracy most of the times. 

<h1>§6. Score on Test Data</h1>

Now we would want to evaluate the accuracy of our most performant model, model4, on the unseen test_dataset


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 68ms/step - loss: 0.0431 - accuracy: 0.9896
    Test accuracy : 0.9895833134651184
    

We see that the test accuracy is over 98% which is quite impressive!
That will be all for today~
